# 第三次创新实践答辩



## 校园AI导航

在南科大的教育环境中，我们提出了一个创新的项目——基于人工智能的校园导航与服务系统。该系统不仅仅是一个简单的导航工具，它结合了大数据、人工智能技术以及校园特有的文化和需求，提供了一系列定制化服务。以下是我们项目的详细介绍和潜在作用：

1. **数据收集的全面性**：我们团队收集了南科大丰富的数据集，涵盖了校园生活的方方面面。这些数据包括教师信息、课程安排、校园建筑资料、餐饮服务信息等，全面覆盖用户的各种需求。
2. **高质量人性化问答**：我们使用先进的大语言模型作为核心，提供高质量且人性化的问答体验。用户可以通过自然语言提出问题，系统能够准确理解并给出相关的答案和建议。
3. **综合性服务的提供**：
   - **教师授课评价服务**：学生可以快速获取关于教师授课风格和课程评价的信息，帮助他们选择合适的课程。
   - **教师履历成就介绍服务**：提供教师的详细履历和学术成就介绍，为学生选择学术导师提供重要参考。
   - **校园新闻介绍服务**：及时更新校园新闻和活动，帮助学生快速了解校园动态。
   - **地图导航及沿途地标建筑科普服务**：提供校园地图导航，并在沿途介绍重要地标和建筑的历史与文化背景。
   - **餐厅推荐服务**：根据开放时间、地理位置、菜品种类等因素，为学生推荐合适的餐厅。
4. **潜在作用**：
   - 为新生提供富有趣味且人性化的校园导航，帮助他们快速熟悉校园环境。
   - 提供学术导师的详细信息，为学生选择导师提供方便快捷的途径。
   - 帮助学生了解校园的历史和文化，增强他们对学校的认同感和归属感。
5. **集成化、便捷化、高效性**：此平台将所有服务集成在一起，用户可以在一个界面上获取所有所需信息，提高了使用的便捷性和效率。无论是寻找特定地点、了解校园新闻，还是获取教师信息，用户都能在此平台上快速得到满意的答案。

总之，此项目的实施不仅将极大地丰富和便利南科大师生的校园生活，还将成为学校智能化、信息化建设的重要一步。通过集成化、便捷化的服务，我们旨在打造一个高效且智能的校园向导平台，为校园生活增添更多的便利和乐趣。



## 大语言模型LLM 

大语言模型是指使用大量文本数据训练的深度学习模型 ，可以生成自然语言文本或理解语言文本的含 义。它的特点是规模庞大，包含数十亿的参数。这些模型通常基于深度学习架构，如转化器，这有助于 它们在各种NLP任务上取得令人印象深刻的表现。语言模型可以处理多种自然语言任务，如文本分类、 问答、对话等，是通向人工通用智能的一条重要途径。大语言模型因为其庞大的参数量级以及训练中使 用到的海量文本数据，使得其能够学习语言数据中的复杂模型与知识，展示出一定的常识和推理能力。 大语言模型所展现出来的这种能力，正是我们在校园AI向导项目中希望能够赋予AI向导的能力，因为我 们将深度研究大语言模型背后的运行机制，并尝试制造属于我们项目的大语言模型。 





## 为什么不从头开始训练

虽然理论上来说，完全由南科大以及我们想要的各类数据集训练出来的大模型能最符合我们的需求。但是从头开始训练一个大型模型，如基于GPT-4或类似架构的模型，对于我们这样的小团队来说是很不切实际的，理由如下：

### 1. 数据收集和处理

- **挑战**：需要大规模、多样化且高质量的数据。这包括文本、图像、音频等不同类型的数据，以及来自不同领域和语言的资料。
- **细节**：数据不仅要多，还要好。需要包括各种主题、风格、语言和格式，保证模型的全面性和多样性。此外，数据还必须是干净的，即无错误、偏差或冗余，这需要大量的数据清洗工作。

### 2. 计算资源

- **挑战**：需要极其强大的硬件支持。这通常指高性能GPU或TPU集群。
- **细节**：大型模型需要成千上万的GPU核心进行并行处理。这不仅意味着高昂的购置成本，还包括维护、电力和冷却系统的成本。

### 3. 专业知识

- **挑战**：需要深厚的机器学习、人工智能和自然语言处理的知识。
- **细节**：涉及模型架构的设计，训练过程中的参数调优，以及对模型性能的细致评估和调整。

### 4. 时间成本

- **挑战**：训练过程可能持续几周甚至几个月。
- **细节**：即使拥有足够的计算资源，模型的训练周期仍然很长，这期间需要持续监控和调整。

### 5. 能源消耗

- **挑战**：巨大的能源需求和相应的碳足迹。
- **细节**：高性能计算资源的运行需要大量的电力，这不仅增加了成本，还可能带来环境问题，如高碳排放。

### 6. 财务成本

- **挑战**：高昂的硬件购置和维护成本，以及可能的云计算资源费用。
- **细节**：硬件的初期购置只是开始，还有持续的电力和维护成本。另外，云计算虽然是一个替代方案，但长期使用也会产生显著费用。

### 7. 模型调优和验证

- **挑战**：确保模型的准确性和可靠性需要大量的测试和调优。
- **细节**：这涉及到各种性能指标的评估，如准确率、召回率、误差率等。同时，还需要不断地调整模型参数，如学习率、层数、隐藏单元数等，以达到最佳性能。



## 为什么微调选择LoRA（QLoRA) 

以往最常见的微调方式是**fine-tuning**，即全参数微调。全参数微调的性能潜力往往更高（因为它可以调整所有的参数），定制化能力也更强，并且实现起来相对简单。但是其最大的问题是它会耗费大量的计算资源和时间（前者往往更致命）。即便在尽可能压缩的情况下，微调一个65B模型也需要耗费两百多GB的显存。这无疑是不可能接受的（即便是使用80GB显存的A100，也需要至少三块）$^{18}$。

随着科技的进步，人们逐渐探索出了其他的微调方法。例如固定住原来预训练模型的参数不变，只对新增的 Adapter 结构进行微调的**Adapter Tuning**$^{19}$；利用人工构建的离散模板（patterns）和提示（prompts）来指导模型更好地理解和处理特定的任务的**Pattern-Exploiting Training**；在模型输入前添加一个连续的且任务特定的向量序列（prefix），在下游微调时，LM的参数被冻结，只有prefix部分的参数进行更新的**Prefix Tuning$^{20}$**；还有放弃了使用自然语言构成的模板，将模板转化为一组连续的可学习参数，并使用一种自适应的剪枝策略，对大型语言模型中的参数进行裁剪，去除其中不必要的冗余参数的**P-Tuning$^{21}$**。图一展示了，在P-Tuning的prefix部分，每一层transformer的embedding输入都需要被tuned。


![1](https://github.com/MikAilis/CXSJ-III-Sustech-LLM-navigation/assets/100832845/60e6c9d4-abea-46dd-8fef-abdd89ed7d1a)


<center>图一</center>



尽管这些方法在内存空间和微调性能上逐步达到了一个合理的平衡，但它们依然存在着诸多不足，比如往往需要引入可训练的prompt token来改进性能，这会导致模型参数数量的增加和可能的增加训练的复杂性和时间。或者关于prompt的设计或多或少还是会依赖于特定的项目，这也会降低泛化性。如此便引出了我们采用的微调方法：**LoRA$^{22}$**，其全称是**LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS**。

LoRA的核心思想是，用一种低秩的方式来调整这些参数矩阵（它冻结了预训练的模型权重，并将可训练的秩分解矩阵注入到 Transformer 架构的每一层中，大大减少了下游任务的可训练参数的数量）（大致过程可参照图二）



![2](https://github.com/MikAilis/CXSJ-III-Sustech-LLM-navigation/assets/100832845/f6c9714a-97b2-4335-bf72-fdfc11993c03)


<center>图二</center>

详细的过程可拆解为如下四步：

1. 对于模型中的每个权重矩阵$W$，而不是直接对$W$进行修改，我们维持$W$不变。

2. 我们引入两个新的矩阵$U$(通常称为更新矩阵)和$V$(通常称为投影矩阵)，并训练这两个矩阵来学习权重的修改。

3. 在模型的前向传播中，原始的权重矩阵$W$被替换为$W＋△W$，其中$△W =UxV$。这个$△W$代表对$W$的修改，而由于$U$和$V$的维度，$△W$是一个低秩矩阵。

4. 这样，即使$△W$是低秩的，它也可以捕捉到$W$的重要变化。并且，由于$U$和$V$的参数数量远少于W的参数数量，所以这种方法能够减少需要训练的参数总数。



对于前面的**显存占用过大**以及**训练时间过长这两个问题**，LoRA都有良好的解决效益。另外，LoRA还可以带来一些额外的惊喜：

* 虽然LoRA在模型中引入了新参数和计算，稍微减慢了前向传播的速度。但另一方面，GPU之间需要进行的梯度通信较少，加快了反向传播速度。并且得益于秩分解，这两个低秩权重矩阵的参数量相比于原先的权重矩阵根本不在同一个量级上，故而时间可以大幅度缩短。
* 显存占用方面，也因为改写的参数量从 $d×d$ 降低至 $2×r×d$（即在一个epoch的训练完成后，只需保存这些低秩矩阵的部分），所以相对于全参数，LoRA的显存占用率也有了质的突破（权威测量如图三所示）。同时经过我们的测试，对于一个**7b**的模型（internlm-7b），在**float16**精度下，微调占用的显存也仅有**17GB**左右。（相较于全参数微调的**50GB**，的确显著减少了）
* 与常规checkpoint相比，LoRA的checkpoint明显更小（对于一个7b的模型，每个checkpoint大约只有**0.89GB**的大小，明显小于全参数的checkpoint）。这更便于扩展服务，特别是在管理多个经微调的模型时。并且如果我们想测试一轮训练中不同epoch的情况时（比如对寻找最佳loss的时候），这样大小和规模的checkpoint将能更加降低存储空间损耗，以及模型合并耗费的时间。

![3](https://github.com/MikAilis/CXSJ-III-Sustech-LLM-navigation/assets/100832845/9ca07b36-8d98-4c52-9f55-83d75a3aecc8)


<center>图三</center>

而**QLoRA**$^{23}$，这种能够实现高保真的4位微调的微调方法，则在此基础上，添加了：

* **4-bit NormalFloat Quantization（减小数据范围，但是尽量保持数据原有的信息。将连续的值映射到一个有限的离散集合中）**
* **Double Quantization（减少神经网络参数所需存储空间的技术，通过量化参数以减少每个参数需要的位数，进而减少模型的存储大小和加速计算）**
* **Paged Optimizers（利用了NVIDIA GPU的统一内存特性。允许在GPU内存不足时，自动将数据传输到CPU内存，并在需要时重新传回GPU）**

使得显存占用率进一步降低。

故而，权衡利弊后，在微调上，我们采用了LoRA及其变体的微调手段。



## 实际实验

 首先介绍下我们的部分参数选择：

> $r = 32$  (秩数，它影响模型参数量和其能够控制内容的粒度。因为我们数据集（100K）较小，同时显存空间有限，故而选择32这个中间值)
>
> $alpha = 64$ (缩放因子，通过这个值与LoRA Rank的比例，可以调整LoRA层的强度。我们希望LoRA层的影响力更强，所以选择了两倍)
>
> $dropout = 0.1$ (神经元随机丢弃的概率，用于防止模型过拟合。这是默认推荐值。我们也尝试了其他值，但效果不尽人意，最终选择原封不动)
>
> $maxLength = 2048$ (最大拼接长度。在实际微调中，模型会根据这个值，根据贪婪算法将数据集拼接起来。结合显卡实际现状，选择了2048)
>
> $lr = 1e-3$ (学习率，通常来说$1e-5$ 到 $1e-2$ 之间的值是一个不错的选择。经过反复测试，我们发现$1e-3$的表现相对较好)
>
> Optimizer：adamw_torch (adam优化器一直以来被广泛使用，而torch变种更能结合权重衰减的优点，有助于更有效的训练过程和减少过拟合)
>
> max_new_tokens = 256 
>
> temperture = 0.7 
>
> top_p = 0.95 





我们本周期的测试目标主要集中于数据集上。

数据集统一采用了**alpaca**格式，以满足南方科技大学AI向导在执行事实类问询任务中对数据的特定需求。鉴于AI向导主要处理的是直接的查询，这些查询通常不涉及长篇的对话，而是集中在简洁、精确的单轮问答上。这样的格式设计旨在优化AI对于用户输入的快速响应与信息的精确抓取。当前的工作重心是在AI任务处理中精确地回答基于事实的问题，例如对南科大教师的专业领域、课程内容、学术成就等方面的直接询问。此步骤是构建一个更为全面的问答系统的基础，它需要能够从提供的信息中快速提取关键事实，并生成直接、相关的答案，以此来满足用户的即时查询需求。格式的示例如下：

```json
"conversation":[

    {

      "system": "(指定的大模型身份，让大模型更能了解此时的任务需求)",

      "input": "(用户可能的提问)",

      "output": "(要求的大模型的回答)"

    }

  ]
```

数据集的定义如下：

* **original_data**：这是从南方科技大学官网计算机系上爬取的对教师的介绍的数据集，总大小约**50KB**。对每个教师分为两个部分，一为介绍教师（包括其基本信息，履历和成就等）（后面简称为**任务一**），二为询问哪里可以找到教师（会返回教师的邮箱）（后面简称为**任务二**）。我们想借由此来观察大模型的**机械信息泛化能力**和**指定信息检索回答能力**。
* **duplicate_data**：这是将**original_data**复制粘贴四次后的数据集。我们想借由此来观察，多次重复的数据集会对大模型的**loss**和**拟合能力**产生什么样的影响。
* **mixed_data**：这是在**original_data**的基础上，添加了从**Niuwa Curriculum Evaluation System**爬取的学生对教师课程的评价的信息的数据集，总大小约**100KB**。另外，我们还把单调的教师信息问答模式进行了调整，使得其在保持原有信息含义的情况下，样式更加多元化。在我们的设想中，南科大ai向导不应该只会给出冷冰冰的机械性回答，而应该能像朋友一般给出人性化，口语化的回答，所以加入了这样较为随意和富含网络用语的数据集。在构建数据集的过程中，我们注意到部分评价有失得体，过于极端，可能会对教师风评产生不应该的不当影响，故而进行了有选择性的数据清洗。
* **updated_data**: 这是纯粹的由**Niuwa Curriculum Evaluation System**中的学生评价构成的数据集，大小约**120KB**。我们想借由此来观察大模型的**信息综合提取和总结能力**。



对数据集的验证如下：

* **基本问题**：我们选择了十个直接从微调数据集中复制过来的问题进行测试（五个询问教师信息，五个询问哪里可以找到教师），并且按点给分。
* **过拟合测试**：为了避免过拟合带来的上面基本问题的高正确率，我们添加了四个钓鱼问题，其格式和基本问题基本类似，但是和教师完全不沾边（比如，在基本问题中的”我在哪里可以找到xxx老师，在钓鱼问题中被替换为了“我在哪里可以找到唐纳德特朗普”和“我在哪里可以找到三体人”）（后面简称为**任务三**）



我们分为了如下几个测试模块：

### 1. 重复性测试

本次测试对比了**original_data**和**duplicate_data**，选择的base model为internlm-20b。对于**duplicate_data**，因为其在第20个epoch的loss便基本接近0了，故而只训练了20个epoch（剩余的数据由第20个epoch的复制填充）。而**original_data**则是进行了50个epoch的训练。loss和grad_norm（梯度范数）的对比图可参考图四和图五。

![4](https://github.com/MikAilis/CXSJ-III-Sustech-LLM-navigation/assets/100832845/7a7947e2-0891-4fd9-9717-d67de366ee13)

<center>图四</center>

![5](https://github.com/MikAilis/CXSJ-III-Sustech-LLM-navigation/assets/100832845/8029ab75-7f16-4fb3-8670-9fdf5cee01bf)

<center>图五</center>

结合两份图表，可以发现对于**duplicate_data**，它在第6到15个epoch间loss迅速下降，并且参数变化率极大，这表明模型从重复数据中学习了非常多并迅速改进其预测。但作为代价，我们推测，在第15个epoch以后的模型里，它的**过拟合现象应当相当严重**。而对于**original_data**，它的训练曲线相对缓和，没有某一阶段的grad_norm的急剧变化，loss也是平稳的下降。我们推测它的学习能力应该较好。

实际测试结果也印证了我们的推测。在**duplicate_data**的第10个epoch的lora的merge后的模型推断中，它对于任务二的回答的正确率为**100%**，对于任务一的回答符合指定的格式，正确率也有**40%**左右（存在混杂问题）。但是在任务三中，它对于”我可以怎么找到三体人“的回答是：“对不起，我没有他们的地址。但是，你可以通过电子邮件来联系他们：trisolaris@sustech.edu.cn。”这显然就是过拟合造成的结果。此外，我们还观察到，后续的epoch中，即便**它的过拟合现象越来越严重**，但是**它对于任务一的回答准确率却基本没有提升**。

在**original_data**中，即便到了第50个epoch，它也**不存在任何过拟合问题**。并且，**它的回答格式相对更加灵活，不是只拘泥于我提供的样化范式**。例如，我指定的：”对不起，我没有他的地址。但是，你可以通过电子邮件来联系他：xxx。“大模型在实际回答中替换为了：“您可以通过南方科技大学的官方网站来查找xxx教授的联系方式。在南方科技大学的计算机科学与工程系页面上，您可以找到xxx教授的简介和联系方式，包括他的办公室地址、电子邮件地址和电话号码。此外，您还可以通过电子邮件地址xxx来直接联系他。希望这些信息能够帮助您找到xxx教授。”

但是，在**original_data**中，即便loss已经降到接近于0，它回答的正确率依然很低。对于任务二，仅有**60%**（表现为，**它知道邮箱后缀和以名字首字母开头的命名方式，但前面的数字会忘记**）。对于任务一，则仅有**10%**左右（将多个教师的介绍**混杂到了一起**）。



### 2. 丰富性测试

本次测试对比了**original_data**和**mixed_data**，选择的base model为internlm-7b。每个数据集均训练了100个epoch。loss和grad_norm的对比图可参考图六和图七。（其中**mixed_data**后缀为more）

![6](https://github.com/MikAilis/CXSJ-III-Sustech-LLM-navigation/assets/100832845/13ef222e-f32d-4abb-bc19-8217db0de56a)


<center>图六</center>

![7](https://github.com/MikAilis/CXSJ-III-Sustech-LLM-navigation/assets/100832845/0b73031f-f9d9-4a28-9d59-5ab82bd55b40)


<center>图七</center>

结合两张图表来看，**mixed_data**的loss和gran_norm变化似乎都比较正常，都在相对平衡地变化。并且由于它的loss曲线下降更为平滑和迅速，我们推测在结合了多元化的数据后，它的学习能力更强。

但是实际结果证明了我们的猜想的错误。对于**mixed_data**的第15个epoch，即便它的loss仍然很高，而且实际验证时总体正确率只有**5%**左右（对于所有询问教师信息的问题，它全都回答了不知道），它也依然出现了过拟合的情况。表现为，对于“ 我可以怎么找到唐纳德川普”的问题，它的回答是：“对不起，我没有他的地址。但是，你可以通过电子邮件来与他联系：dcunh@sustech.edu.cn。这是他的邮箱地址，你可以在南方科技大学的网站上找到它。”

在**mixed_data**后续的epoch中，对于任务二的表现反而相对良好，正确率达到了**100%**，但是任务一甚至不如**original_data**在同等epoch下的表现。并且，前面为了提升人性化和口语化所做的举措似乎**完全没有任何效果**。在第15到第50个epoch所merge的模型中，**在同一轮回答中，它所有的回答模板都是固定统一的，并且十分生硬**。我们猜测原因可能如下：

* **数据不平衡和数据集大小**：**original_data**数据集相对较小（50KB），而**mixed_data**则是其两倍大小。如果在**mixed_data**中，某一类数据（如教师评价）比其他类别（如教师基本信息）更多，这可能导致模型在训练时对这一类数据过度拟合，而忽视了其他类型的数据。数据不平衡可能导致模型无法有效学习多样化的回答风格。
* **数据复杂度和多样性**：**mixed_data**不仅包括基本信息，还包括更多样化和随意的内容，如学生评价和网络用语。这种复杂性和多样性的增加可能使模型难以在有限的训练周期内有效学习和泛化。
* **数据清洗的影响**：对于**mixed_data**，你们进行了选择性的数据清洗，特别是删除了一些极端的评价。这种清洗可能影响了数据的代表性和多样性，进而影响模型的学习能力。



### 3. 信息提取测试

本次测试对比了**updated_data**和**mixed_data**，选择的base model为internlm-7b。每个数据集均训练了100个epoch。loss和grad_norm的对比图可参考图八和图九。（其中**updated_data**后缀为real）本次饰演所采用的验证数据集改为随机抽取的微调数据集中的教师评价。

![8](https://github.com/MikAilis/CXSJ-III-Sustech-LLM-navigation/assets/100832845/f7eee60c-c013-4753-93e6-59ba7fbb4457)


<center>图八</center>

![9](https://github.com/MikAilis/CXSJ-III-Sustech-LLM-navigation/assets/100832845/66aeeb07-8445-40fa-b328-922d468bdf25)


<center>图九</center>

结合两张图表，**updated_data**的学习曲线更为平缓，参数更新也较为正常。

但是在实际测试中，我们还是发现，即便又到了过拟合的情况（比如第70个epoch），模型回答正确率依然极低。它不仅依然有先前两次实验中，**将多个预期输出混杂的情况**，甚至**出现了非常严重的胡编乱造的情况**（数据集中完全不存在这样的评价和内容）。例如对于某体育老师，它给出的答案是：“他是南科大很厉害的老师之一，教机械设计也很不错，教学水平高，给分也特别好，特别推荐。”

除此之外，我们预想中**对于信息提取的能力也完全没有得到应验**。当询问一名教师的课程如何时，大模型只是简单地把多个评价糅杂在一起，给出了一条多次重复同一含义的回答。例如对于某英语老师，它的答案是：""他是南科大很厉害的老师之一...他很厉害...他很厉害..."。而且它**也更倾向于只回答这样笼统概括性的话语，而不是一些学生对老师具体做法的概述和评价**。

尽管如此，我们依然注意到，在这样的数据集下，大模型的回答**的确更加口语化和人性化了**。例如，有些表示亲昵的词语和网络用语也被应用到了大模型的输出中（即便我并没有在这个输入的对应输出中添加类似的用语）。



### 4. 参数量对比测试

对于前面三个测试中的后两个，我们也有考虑过是不是因为7b的模型相对太小而导致表现不佳。因此我们还进行了对7b和20b模型的比对，但是通过对图表的分析（可以参照测试一和测试二中的图表）和同一epoch下实际结果的正确率的对比，我们发现，起码在这样对较小数据集的微调上，模型参数的不同所导致的差异微乎其微。所以可以排除掉这一因素。





## 分析

在初期的预想中，我们认为微调可能是一种万能的解决问题的手段，考虑到微调中的指导调整已经被证明了可以处理医学、法律、金融等专业领域的知识，因此，指导调整也应该可以处理南科大教师相关的专业知识。但是前述的实验结果表明了指导微调的效果并不如预期。结合相关文献$^{24}$，我们得到了一些启示。其中的第34-35页揭示了微调的主要作用：

* **任务泛化：** 指导调整使LLMs能够理解并遵循自然语言指令来完成各种任务，包括未见任务。这种方法已被证明在已见和未见任务上都能够提供出色的性能，同时还可以缓解LLMs的一些弱点，如生成重复内容或未完成任务。指导调整还可以使LLMs在不同语言之间泛化到相关任务，甚至在多语言任务上使用英文指令也可以产生令人满意的结果。
* **领域专业化：** 指导调整是将通用LLMs调整为特定领域专家的有效方法。研究表明，通过在特定领域的数据上进行微调，LLMs可以成为医学、法律、金融等领域的专业工具，实现与专业人士相媲美的性能。这种方法还可以应用于电子商务推荐系统等应用中，以支持特定领域的任务。

并且结合表现各个测试中的表现特征后，我们得出了一个结论：

**大模型的微调，重点不是在于提升具体某些回答的准确性，而是让大模型能够以更加专业的角度，更加相关的分析，和更符合相关设定的口吻来回答某些问题**

具体表现在测试中，则是：

1. 它学习了南科大邮箱的命名规则。
2. 它学习了如何更加客观广泛地评价一个教师，并且知道了可以从哪些角度来评价。
3. 它学习了如何以更符合学生口吻的方式来评价老师的课程。

另外，在查询相关文献后$^{25}$后，我们也得到了一些作证。例如行业SOTA的chatGPT4，在处理维基百科的相关问题时（GPT4预训练的其中一个数据来源就是维基百科）（比如以其为数据集的多选题），它的正确率也只能达到**86.4%**。并且，这还是在经过诸如环环提示等方法得出来的结果。

经过以上分析，我们认为，如果要达成我们南科大ai向导的目标，光靠微调是不够的，因为我们需要更高的准确性。但是抛弃微调也是不明智的，因为我们需要让大模型知道南科大ai向导应该如何友善地、亲切地、口语化地与同学们交流沟通。

最终，我们将提高准确性的解决出路，定为**langchain + 知识图谱**。



## langchain + 知识图谱 + llm

在先前的汇报中，我们提到**知识图谱**虽然是一种高准确率的信息检索手段（例如被广泛应用于浏览器搜索中），但是它的可拓展性差，对输入数据格式要求高，并且返回内容单一乏味。但是如果能结合大模型的话，这些问题都能得到有效的解决，而**langchain**，这一**旨在帮助开发人员使用语言模型构建端到端的应用程序的强大且便利的框架**，更能为我们提供一个良好的对接平台。



### 可行性分析

今年（2023年）一篇文章$^{26}$便指出了用知识图谱增强 LLM具体的方式：

1. 使用知识图谱增强 LLM 预训练，其目的是在预训练阶段将知识注入到 LLM 中
2. 使用知识图谱增强 LLM 推理，这能让 LLM 在生成句子时考虑到最新知识
3. 使用知识图谱增强 LLM 可解释性，从而让我们更好地理解 LLM 的行为

虽然这些方法能够有效地将知识与LLM中的文本表示进行融合。但是，真实世界的知识会变化，这些方法的**局限是它们不允许更新已整合的知识**，因此在推理时，它们可能无法很好地泛化用于未见过的知识。一种解决方案是对模型重新微调，但这样相对会耗费大量时间和算力资源。而知识图谱可以**保持知识空间和文本空间的分离，并在推理时注入知识**。

一个良好的框架如图十所示。大致运行机制如下：

* 首先，LLM编码器接收格式化的问题和选项，并将其转换成深层次的语言模型表示。
* 同时，KG编码器利用知识图谱中的结构化信息来提供背景知识。
* 这两种信息通过联合推理层进行整合，这一层使用注意力机制在语言模型表示和知识图谱之间建立联系。
* 为了提高效率和准确性，动态剪枝机制在推理过程中评估并筛选出最有可能的答案选项。
* 最终，答案推断部分基于联合推理结果生成最终答案，充分利用了语言模型的理解能力和知识图谱的丰富信息。

总体而言，它利用语言模型的强大文本理解能力，并且通过知识图谱获得结构化知识的支持，以更准确地回答复杂的问题。



![10](https://github.com/MikAilis/CXSJ-III-Sustech-LLM-navigation/assets/100832845/b847d751-7a9d-44cd-a3cb-017c7efb7c80)


<center>图十</center>



### 架构构建

对于方案设计，我们决定参考如图十二所示的架构$^{27}$：



<center>图十二</center>

![12](https://github.com/MikAilis/CXSJ-III-Sustech-LLM-navigation/assets/100832845/71ba1240-2ac3-414d-9e39-d0da93f1f782)


#### LLM

大模型层的主要功能可以分为如下几点：

* **对用户问题的预处理。纠正语法错误，提取关键点，通过交互方式引导用户补充问题足够多的信息**。比如，如果一名同学询问“请告诉我张进老师的信息”，在实际检索知识库后，大模型会发现南科大有两个张进老师，它就会返回“请问您想知道数学系的张进老师，还是计算机系的张进老师？”。等到用户补充完整信息后，大模型会将两次对话得到的信息提取拼接起来，从而向本地知识库提出“计算机系的张进老师”的检索诉求。
* **对检索结果的再处理**。比如，如果一名同学询问“请告诉我计算机系的张进老师的信息”，而知识库返回了长达两三百字的张进老师的成就信息，这无疑会对同学造成极大的困扰。超出某个阈值后，大模型就会对这样的信息进行再处理，提取浓缩为便于阅读的较短的信息返回给用户。



#### 查询系统

计划采用**Embedding-based search**$^{28}$的方式，将文字形式的查询请求，编码为数值向量的形式，体现潜在的关系。即，将用户的请求进行 Embedding，获得向量。然后使用问题向量在知识库中搜索，找到与之最匹配的若干记录。将这些记录的原始材料返回。



## 与单纯LLM对比

知识图谱主要的问题在于结构化，涉及到: 关系结构化、内容输入结构化、查询结构化。

当知识图谱存储内容很多的时候，根据用户上下文和最近的问题，来查询知识图谱内容，一定会涉及到构建一次查询语句。

而大模型的核心就是非结构化。训练过程和推理的时候，都不需要做结构化，而且实时开始回复。

| 对比项           | 知识图谱 + 大模型                                            | 纯大模型                               |
| ---------------- | ------------------------------------------------------------ | -------------------------------------- |
| 训练工作         | 构建知识图谱关系结构化、内容结构化                           | 语料筛选和大模型训练                   |
| 使用时流程       | 大模型推理生成知识图谱查询语句，知识图谱内容查询；将查询出来的内容放到prompt中，再进行一次大模型推理 | 大模型推理一次                         |
| 用户感知反馈延迟 | 平均几秒左右                                                 | 立即反馈                               |
| 后期升级维护     | 构建新的图谱关系、结构化内容，需要大量人工，无法全自动化     | 增加语料，再次训练，无需人工，全自动化 |

知识图谱+LLM的主要优势在

1. 知识细粒度的可操作性
2. 实时可更新性
3. 知识来源的可追溯性
4. 相比纯LLM幻觉会更少·

### 知识细粒度的可操作性

可操作性是指可以很方便地添加或删除、更新知识，这些内容在单纯LLM中难以显示做到。

知识细粒度是知识图谱的上述操作可以精确到每一条知识，甚至到知识的entry/relation,这是大模型做不到的。

### 实时可更新性

这个优势来自于上述的细粒度可操作性。大模型的更新要求足够数量的新数据如果数量太少的话则几乎无法改变模型学习到的distribution; 而知识图谱则可以在数据流上实时进行知识的更新，哪怕只有一条新数据也可以更新。

### 知识来源的可追溯性

知识图谱回答问题用到的每条知识都可以记录其来源(比如文献、链接等)，并在回答问题时一并提供，便于用户对答案进行正确性确认。而大模型做不到这一点

因此，知识图谱的特点是: 

- 结构化知识，支持基于字面的匹配，比较rigid、泛化推广能力不强
- 快速更新数据，可用于实时的知识过滤器，比如快速查找最新的新闻热点和关键词
- 知识可追溯

而大模型的特点是: 

- 学习知识的数据分布，支持基于语义 (semantics) 的匹配，关键词匹配能力稍弱，但泛用性强，可以针对unseen question进行回答。 
- 知识不可追溯，可能存在幻觉 (hallucination) 问题



## 幻觉

当模型生成的文本不遵循原文（Faithfulness）或者不符合事实（Factualness），我们就可以认为模型出现了幻觉的问题，也就是当前我们遇到的主要问题所在。

- **Faithfulness**：是否遵循input content；
- **Factualness**：是否符合世界知识
  ![image-20231202214931919](https://github.com/MikAilis/CXSJ-III-Sustech-LLM-navigation/assets/100832845/64a0d9f2-18e6-413b-93d7-58dce6a9c45e)

**幻觉的差异**

- **数据源（source）不一致**：

- - 例如：摘要的数据源是document，data-to-text的数据源是data table，对话的数据源是对话历史，而开放域对话的数据源可以是世界知识。

- **容忍幻觉的程度不一致**：

- - 在摘要、data-to-text任务中，非常看重response的Faithfulness，因此这些任务对幻觉的容忍程度很低；
  - 而像开发域对话任务中，只需要response符合事实即可，容忍程度较高

**在传统任务里，幻觉大都是指的是Faithfulness：**

- **Intrinsic Hallucination（信息冲突）**: LMs在生成回复时，与输入信息产生了冲突，例如摘要问题里，abstract和document的信息不一致。
- **Extrinsic Hallucination（无中生有）:** LMs在生成回复时，输出一些并没有体现在输入中的额外信息，比如邮箱地址、电话号码、住址，并且难以验证其真假。（PS: 按照此定义，Extrinsic Hallucination有可能是真的信息，只是需要外部信息源进行认证）

**而面向LLMs，我们通常考虑的幻觉则是Factualness：**

- 因为我们应用LLM的形式是open-domain Chat，而不是局限于特定任务，所以数据源可以看做任意的世界知识。LLMs如果生成了不在input source里的额外信息，但是符合事实的，这种情况也可能是对我们有帮助的。



## 幻觉的原因

### 数据层面

在数据工程层面可能出现一些问题，导致幻觉问题：

- 训练数据收集过程中，众包/爬虫检索的数据可能包含虚假信息，从而让模型记忆了错误的知识；
- 过多的重复信息也可能导致模型的知识记忆出现bias，从而导致幻觉[1]

**潜在的研究方向：**

- Building High-quality Training Corpus is essential.
- Data verification/ Data filter/ Data selection.

### 模型层面

即使有了高质量训练数据，LLMs仍然可能表现出幻觉现象。

- **模型结构**：如果是较弱的backbone（比如RNN）可能导致比较严重的幻觉问题，但在LLMs时代应该不太可能存在这一问题；
- **解码算法**：研究表明，如果使用不确定性较高的采样算法（e.g.，top-p）会诱导LMs出现更严重的幻觉问题。甚至可以故意在解码算法中加入一些随机性，进一步让LMs胡编乱造（可以用该方法生成一些negative samples）[2]

- **暴露偏差**：训练和测试阶段不匹配的exposure bias问题可能导致LLMs出现幻觉，特别是生成long-form response的时候。[3]

- **参数知识：**LMs在预训练阶段记忆的错误的知识，将会严重导致幻觉问题。[4]



## 幻觉的评估

现有的传统幻觉评估指标和人类结果的相关性往往较低，同时大都是task-specific的。[5]

### Reference-based

Reference-based的指标有两类：

- **基于Source Information和Target Reference**：利用一些统计学指标，比如ROUGE、BLEU来评估输出结果和Source/Target信息的重叠度。
- **基于Source Information**：由于NLG任务里，Target输出往往是多种多样的，因此许多工作只基于Source信息进行幻觉的评估。比如Knowledge F1。

*基于Reference的评价指标，基本上只能评价Faithfulness，而无法评价Factualness，因此通常不适用于LLMs。*

### **Reference-Free**

**基于IE**：将知识限定于可以用三元组形式表示的关系和事件，基于额外的IE模型进行抽取，接着使用额外模型进行验证

- **缺点：**
  - 可能存在IE模型的错误传播问题。
  - 知识被限定在三元组形式。

**基于QA**：第一步先基于LM生成的回复，使用一个QG(question generation)模型生成一系列QA pairs；第二步给定Source Information，让QA模型对上一步生成的Question进行回复；第三步则是通过对比第一步的answers和第二步的answers，计算匹配指标，衡量模型的幻觉问题；

- **缺点**：
  - 同样存在QA/QG模型的错误传播问题。
  - 难以评估Factualness，因为上述第二步里面，Source Information不可能包含全部的世界知识，因此对于一些问题难以生成可靠的回复。[6]

**基于NLI**：基于NLI的方法通过NLI模型评估是否Source Information可以蕴含Generated Text，从而评估是否出现了幻觉现象。

- **缺点**：
  - Off-the-shelf NLI模型用于核查事实效果不是很好；[7]
  - 无法评估需要世界知识的幻觉问题：仅能依赖于Source进行核查；
  - 都是sentence-level的，无法支撑更细粒度的幻觉检查；[8]
  - 幻觉问题和蕴含问题实际并不等价：
    - 例子：Putin is president. -> Putin is U.S. president (可以蕴含，但是是幻觉)

**基于Factualness Classification Metric**：标注/构造一批和幻觉/事实有关的数据，训练检测模型，利用该模型评估新生成文本的幻觉/事实问题。[9]

**人工评估：**目前为止最靠谱的，此外还可以依靠LLM打分（比如利用GPT4，但是GPT4也存在着严重的幻觉问题，即使经过retrival-augment，检索回来的信息也有可能是错误的）



## 当前幻觉问题分析

### Retrieval-level causes

```
double enter to end input >>> 罗林斯特老师的体育II咋样？
罗老师上课挺有意思的，让我们拿着跳绳动起来，活跃气氛，不过感觉老师对于体育知识可能并不是很专业，问问题老师有时候回答不上来，但是给分应该挺好，上了课的大多数人都选了老师这个方向。 罗老师很专业，曾经拿过国家奖学金，给分也是相当好，好像因为是外聘老师，所以可能没什么存在感，但是实力是有的。 罗老师很厉害，曾经拿过国家奖学金，给分也特别好，而且性格温和，上课也非常有意思，后来还选过他的攀岩课，也很有意思。 罗老师很厉害，曾经拿过国家奖学金，给分也特别好，而且性格温和，上课也非常有意思，后来还选过他的攀岩课，也很有意思。 罗老师很厉害，曾经拿过国家奖学金，给分也特别好，而且性格温和，上课也非常有意思，后来还选过他的攀岩课，也很有意思。
```

现阶段遇到的问题主要出现在信息检索上，当前模型的回答多是factual但是存在幻觉内容。如上述回答，罗林斯特老师虽然是外聘教授，但并未开设攀岩项目，整体上LLM的回答内容是符合现实逻辑的但是在客观信息上存在误差。这一结果体现了微调工作的部分成功，即通过使用南科大数据集的微调，使得模型学习了在学校相关问题上的术语、表达方式和相关内容的处理方法，但微调结果并不一定显著体现在模型的准确性与深度上。



## 幻觉的缓解

### 基于数据的工作

**构建高质量数据集**

1. **人工标注**

2. 1. **训练数据**：LLM上不可行，只适用于task-specific的幻觉问题
   2. **评测数据**：构建细粒度的幻觉评估benchmark用于分析幻觉的严重程度和原因[11]

**自动筛选**：

2. 1. 利用**模型筛选**出可能导致幻觉的数据并剔除；
   2. 预训练时给更faithful的**数据加权**（wiki vs. fake news），或者不使用可靠来源的数据（比如只选用经过人工审查的数据源，如wiki或者教科书，预训练）

### 模型层面的工作

**模型结构**

- 模型结构层面的工作往往focus在设计更能充分编码利用source information的方法，比如融入一些人类偏置，如GNN网络。
- 或者在解码时减少模型的**生成随机性**，因为diversity和Faithfulness往往是一个trade-off的关系，减少diversity/randomness可以变相提升Faithfulness/Factuality。[10]

- **检索增强**被证明可以显著减少幻觉问题，e.g., llama-index。[12]

**训练方式**

- **可控文本生成**：将幻觉的程度作为一个可控的属性，利用可控文本生成技术进行控制。[13,14]
- **提前规划骨架，再生成**：sketch to content[15]
- **强化学习：**假设是基于word的MLE训练目标，只优化唯一的reference，可能导致暴露偏差问题。现有工作将减轻幻觉的指标作为强化学习的reward函数，从而减轻幻觉现象。[16]
- **多任务学习**: 通过设计合适的额外任务，可以达到减轻幻觉的效果。
- **后处理**：设计一个小模型专门用于fix幻觉错误。[17]

## Benchmark

### 模型强弱的核心指标

首先，把一个模型调成一个对话机器人这件事情并不难，开源界已经有了类似于 Alpaca, Vicuna, RWKV 这样的对话机器人，跟它们随便聊聊感觉都还不错；但要真正希望这些模型成为生产力，随便聊聊是不够的。所以构造评价基准的第一个问题是要找到区分度，弄明白什么样的能力才是区分模型强弱的核心指标。我们考虑**知识**和**推理**这两项核心。

### 知识

为什么说知识性的能力是核心能力？有以下几点论点：

- 我们希望模型可以通用，可以在不同领域都贡献生产力，这自然需要模型知道各个领域的知识。
- 我们同时希望模型不要胡说八道，不知为不知，这也需要扩大模型的知识，让它可以在更少的时候说它不知道。
- 斯坦福的 HELM 英文评价榜单中，一个重要的结论是，模型大小与知识密集型任务的效果显著正相关，这是因为模型的参数量可以被用来储存知识。
- 上文已经提到，已有的重要模型，比如 DeepMind 的 Gopher / Chinchilla，在评价的时候几乎只看 MMLU，MMLU 的核心就是测模型的知识覆盖面。
- GPT-4 的发布博客中，首先就是列出模型在各个学科考试上的效果，作为模型能力的衡量标准。

因此，知识型能力可以很好地衡量底座模型的潜力。

### 推理

推理能力是在知识的基础上进一步上升的能力，它代表着模型是否能做很困难，很复杂的事情。一个模型要强，首先需要广泛的知识，然后在知识的基础上做推理。

推理很重要的论点是：

- GPT-4 的发布博客中，OpenAI 明确写道 “The difference comes out when **the complexity of the task reaches a sufficient threshold**” (GPT-3.5 和 GPT-4 的区别只在任务复杂到一定程度之后才会显现)。这说明推理是很显著的强的模型有，弱一点的模型不大有的能力。
- 在 PaLM-2 的 Tech Report 中，BBH 和 MATH 这两个推理数据集被专门列出来讨论划重点。
- 如果希望模型成为新一代的计算平台，并在上面孕育出全新的应用生态的话，就需要让模型能够做足够强的完成复杂任务的能力。

这里我们还需要厘清推理和知识的关系：

- 知识型的能力是模型能力的基础，推理能力是进一步的升华 — 模型要推理也是基于现有的知识图里。
- 知识性任务的榜单上，模型大小和模型分数一般是连续变化的，不大会因为模型小就出现断崖式下跌 — 从这个角度来说知识型的任务更有区分度一点。
- 推理型任务的榜单上，模型大小和模型分数可能存在相变，只有当模型大到一定程度之后（大概是 50B 往上，也就是 LLaMA 65B 这个量级），模型推理能力才会上来。
- 对于知识性的任务，Chain-of-thought (CoT) prompting 和 Answer-only (AO) prompting 的效果是差不多的；对于推理型任务，CoT 显著好于 AO.
- 所以这边需要记住一下，CoT 只加推理效果不加知识效果。在数据集中，我们也观察到了这个现象。

### 问题重点在于知识型与推理型

- 知识型：南科大地标名称，老师信息，校巴班次等
-  推理型：推荐课程，评价，结合新闻给出相关建议等

模型首先要有相关的知识储备，再根据使用者提出的需求进行分析，给出合理的回答

共计准备了50道题目，每题根据回答的不同情况得分分为5种

1. 有基本的回答框架，但是内容完全错误
2. 有基本的回答框架，回答产生部分幻觉，但存在正确内容
3. 能完整地给出具有可读性的答案，回答内容基本正确，但是不够全面，没有复读内容
4. 给出完整，正确的答案，但回答不够简练明了，或缺少逻辑
5. 回答内容正确，角度丰富全面，能给出详细的细节介绍，逻辑贯通

### 初版微调得分

![image-20240116001737537](https://github.com/MikAilis/CXSJ-III-Sustech-LLM-navigation/assets/100832845/6d1641ae-42d6-4974-ad53-b542c8ff11ef)


![image-20240116001722871](https://github.com/MikAilis/CXSJ-III-Sustech-LLM-navigation/assets/100832845/eac17b94-0bf4-4c08-ac99-86088240b5a9)


![image-20240116001715822](https://github.com/MikAilis/CXSJ-III-Sustech-LLM-navigation/assets/100832845/47d8b86c-cbbe-46de-8ceb-fdd0e9ccfabd)


初版微调结束后，模型共计得分70分，其中常识性问题25分得22.5，得分率90%，表现较好；南科大相关知识42.5分得分28分，得分率65.9%；南科大相关推理32.5分得分19.5，得分率60%。发现模型在推理方面能力较为欠缺，对一些问题已经初步学会了回答的框架，可以按照一定的模式进行回答，但是内容准确率有待提高。

### 最终结果

![image-20240116001715822](https://github.com/MikAilis/CXSJ-III-Sustech-LLM-navigation/assets/100832845/c6c39244-9b0a-4aff-a0b4-615d7e75f36b)


![image-20240116001715822](https://github.com/MikAilis/CXSJ-III-Sustech-LLM-navigation/assets/100832845/e9ad60a1-fdcc-4f71-bfe4-a4870cee4b19)


![image-20240116001715822](https://github.com/MikAilis/CXSJ-III-Sustech-LLM-navigation/assets/100832845/33e90b39-e694-46e3-baf1-471909e38c28)




最终微调结束后，模型共计得分80.5分，其中常识性问题25分得22.5，得分率90%，表现较好；南科大相关知识42.5分得分33分，得分率77.65%；南科大相关推理32.5分得分25，得分率76.9%。

## 知识库搭建

### faiss（Facebook AI Similarity Search）：

这是一个高效的相似性搜索和稠密向量聚类库。它是专门为提高大规模向量搜索的速度和精度而设计的，主要用于机器学习和大数据领域。以下是FAISS的一些关键特点：

1. **高效性能**：FAISS能够快速处理大量数据，尤其是在高维数据上进行相似性搜索时的性能表现尤为出色。
2. **支持多种索引类型**：FAISS提供了多种索引类型，包括扁平（暴力搜索）、树形结构、量化等，以适应不同的应用场景和性能需求。
3. **量化技术**：FAISS使用了向量量化的技术来减少内存使用和提高搜索速度，这在处理大规模数据集时尤其有用。
4. **易于集成**：FAISS可以轻松集成到Python和C++的应用程序中，支持与Numpy和其他科学计算库的结合使用。
5. **适用于多种场景**：从图像检索、推荐系统到聚类分析，FAISS都能提供高效的解决方案。
6. **开源**：FAISS是开源的，可以自由使用并根据需要进行修改和扩展。

### 单段文本最大长度：

- 这个参数限定了在处理、分析或存储文本数据时每段文本的最大字符数或词数。
- 在自然语言处理（NLP）中，尤其是在使用深度学习模型（如BERT、GPT等）时，模型通常有一个固定的最大输入长度限制。这是因为模型的内存和计算能力是有限的。
- 适当的最大长度可以帮助确保文本数据被有效处理，同时避免因文本过长导致的内存溢出或计算效率降低。
- 在某些应用中，调整这个参数可以影响处理的精度和上下文连贯性。

### 相邻文本重合长度：

- 当一个长文本被切分成多个小段进行处理时，相邻文本重合长度指的是两个连续文本块之间的重叠部分。
- 这个参数的目的是为了保持文本上下文的连贯性。通过保留一部分重叠内容，模型可以更好地理解文本的上下文，尤其是在文本的边界区域。
- 在一些场景下，适当增加重合长度可以提高模型对于整个文本含义的理解，尤其是在文本块之间有紧密联系的情况下。
- 然而，过大的重合长度会增加计算负担，并可能导致处理效率降低。



我们最终选择了faiss作为切割检索向量库，将单段文本最大长度设定为250，相邻文本重合长度设定为50。这对于大多数自然语言处理模型来说是一个有效的长度，既不会过长导致处理困难，也足以包含充分的上下文信息。并且，通过设定50个单词的重合长度，可以在处理长文本时保持上下文的连贯性。这意味着相邻的文本块将共享一部分内容，帮助模型更好地理解文本块间的联系和过渡，尤其是在分割点附近的文本。这样我们平衡了处理效率和质量。



## 数据集

### 组成部分

**Teacher_data：**

* 南方科技大学官网上，关于计算机科学与工程系系老师的成就介绍和联系方式。
* 总共26条数据，大小为27kb。内容包括名称，主要事迹成就和联系方式。

**Nkd_data：**

* 从某qq群的学生对教师课程的评价网站上，选择性地爬取的高质量学生评价集。
* 总共93条数据，大小为92kb。内容包括学生对教师某门课程的评价和打分。

**Restaurants_data：**

* 从南科手册上面爬取的南科大所有餐厅的详细信息。
* 总共17条数据，大小为3kb。内容包括餐厅名称，地点，营业实践，供应品种，联系方式和经纬度。

**Buildings_data：**

* 从南科手册上面爬取的南科大一些有名的建筑的详细信息。
* 总共10条数据，大小为5kb。内容包括建筑名称，经纬度和建筑介绍。

**News_data：**

* 官网上南科大近几年来的各类新闻。
* 总共5353条数据，大小为18350kb。内容包括新闻标题，名称，类别和具体内容。

### 数据采集

（hr补充）

## 成果展示

### 前端设计

（hr补充）

### 实际体验

1. **基本建筑介绍：**

* 回答准确详细。

<img src="https://s2.loli.net/2024/01/13/Ugw6GmaEN13lqHV.png"/>

2. **基本老师介绍：**

* 多个数据库联合检索查询（老师成就数据集+新闻数据集）

<img src="https://s2.loli.net/2024/01/13/pYWntRAuX5wUs2H.png"/>

3. **新闻介绍：**

* 准确领会用户意图，给出符合用户需求的强相关的回答
* 简洁有力地概括了新闻内容，而不是原封不动地直接照搬回答

<img src="https://s2.loli.net/2024/01/13/AwyCMvKl4kY7oh6.png"/>

<img src="https://s2.loli.net/2024/01/13/iPmpUEzxgy1KeDj.png"/>

4. **教师评价：**

* 根据关键词检索到了对应信息，并做到了基本准确概括多条信息（在csv数据集中是分为了三四条的独立评价）
* 能根据课程内容和学生感想，最后给出适当的总结和评价

<img src="https://s2.loli.net/2024/01/13/2MjHK4VdG5wvu3c.png"/>

<img src="https://s2.loli.net/2024/01/13/A1W9lfL4p2obxU8.png"/>

5. **地图展示：**

* 根据用户提示，自动展示对应建筑的位置

<img src="https://s2.loli.net/2024/01/13/NAkpWHtFJhi4sud.png"/>

6. **进阶服务**：

这里分为了获取附近建筑和餐厅推荐两个子模块。

关于**代码实现**流程，可以分为如下三个部分：

* 先在本地计算：通过前端浏览器服务获取用户实时经纬度，并根据公式判断用户与数据库内的所有建筑的距离。通过python子库获取用户实时时间，比对数据库内所有餐厅的开放时间，判断仍然开门的餐厅。
* 接着发起请求：通过调用api向大模型发出推理请求，同时通过设置prompt提供具体需求，比如不要输出餐厅具体的经纬度，要输出一些个性化的内容。并添加一些额外信息，比如指定大模型回答的语气人性化口语化，指定格式要便于阅读，上下文统一，还有删改各类语病等。
* 最后输出前端：再次调整格式，通过streamlit打印输出到前端。

**获取建筑实例：**

* 通过点击按钮，返回用户与最近的三个建筑的介绍和实际距离。建筑介绍是让大模型根据数据集存储的信息简洁回答的（如果全部展示出来，文字会过多，对用户阅读体验造成不良影响）

<img src="https://s2.loli.net/2024/01/13/VknNMHuIC3jTlKz.png"/>

**获取餐厅实例：**

* 实现思路与前者基本一致

<img src="https://s2.loli.net/2024/01/13/X8nmWuqwridMCHR.png"/>



### 相对优势

**相对单纯的知识图谱**：

1. **输出更加人性化**：结合了大模型的问答系统不仅能提供准确的事实信息，而且能以更自然、更符合人类交流习惯的方式呈现答案。这意味着系统不仅回答“是什么”，还能解释“为什么”和“怎么样”，从而提供更加丰富和深入的交流体验。
2. **便于拓展（联合搜索）**：这种结合方式使得系统在保持知识图谱的结构化信息优势的同时，还能通过大模型接入更广泛的数据源（如互联网搜索）。这样，即使面对知识图谱之外的信息需求，系统也能提供有效的答案，大大扩展了其应用范围。
3. **结合上下文进行额外推断（面对不在数据库中的知识）**：当遇到知识图谱中不存在的信息时，大模型可以通过上下文线索进行推断，提供合理的假设或相关信息。这种能力使得问答系统在处理模糊查询或需求解释的复杂问题时更加灵活和有效。



**相对单纯的大模型**：

1. **准确率更高**：知识图谱提供了精确的、经过验证的事实信息，这可以显著提高问答系统的准确率。当问题直接关联到知识图谱中的实体和事实时，系统可以直接提供准确的答案，而不是依赖于大模型的概率性推断。
2. **检索性能更强**：知识图谱的结构化特性使得信息检索更加高效和准确。对于特定的查询，系统可以快速定位到相关的实体和关系，从而提供快速且精确的答案。这在处理大量数据和复杂查询时尤为重要。



综上所述，将知识图谱与大模型相结合的问答系统，在提供丰富、准确信息的同时，还能以更自然、人性化的方式与用户互动，满足不同类型查询的需求。这种结合方式不仅提高了系统的适用性和灵活性，也为用户提供了更优质的信息获取体验。



### 格式选择

<img src="https://s2.loli.net/2024/01/13/bcXZvm9g5wYEr1o.png"/>



### SOTA对比

当前类似产品的SOTA毫无疑问是GPTs。相较于GPTs，我们认为我们的系统有如下的优势：

1. **检索效率更高**：由于chatGPT本身过于火热，服务器要响应的请求非常多，导致GPTs响应速度较慢，尤其在高峰时期，甚至会出现卡死报错的情况。并且它本身也有次数限制。再者，其服务器在美国，会存在高延迟的棘手情况。
2. **成本效益更高**：GPTs的使用通常涉及高额的API调用费用，尤其是在大量数据处理和频繁请求的场景下。
3. **更强的隐私保障**：在处理敏感数据和私人信息方面，GPTs可能存在一定的隐私风险。
4. **更专注于回答用户问题**：GPTs在回答问题时有时会受到其安全检测机制的影响，导致回答中出现无关内容或废话。

下面是一个实际对比演示：

<img src="https://s2.loli.net/2024/01/13/pcnMNoF8jlYJ7Hf.png"/>

<img src="https://s2.loli.net/2024/01/13/ksXmYjvERM3wunW.png"/>

可以看到，我们的系统有如下的优势：

1. 应该详细介绍的地方十分详细。我们的系统给出了多个学生实际评价的简要概括，这样可以让用户能更清楚地了解到课程评价。而GPTs则只是用过于简短的一两句话，甚至一两个简单的形容词，来回答用户的问题。
2. 该简略的地方也做到了简略。我们的系统专注于回答用户的问题，回应内容贴合用户需求。而GPTs则可能因为安全检测或其他各种各样的原因，会加入一些用户不太喜闻乐见的内容，比如最后“选择课程，除了考虑分数，还应...”。







## 参考文献

[1]*Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. 2021. Deduplicating training data makes language models better. arXiv preprint arXiv:2107.06499 (2021).*

[2]*Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. 2022. Factuality enhanced language models for open-ended text generation. arXiv preprint arXiv:2206.04624 (2022).*

[3]*Chaojun Wang and Rico Sennrich. 2020. On exposure bias, hallucination and domain shift in neural machine translation. In Proceedings of the 2020 Annual Conference of the Association for Computational Linguistics. 3544–3552.*

[4]*Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. 2021. Entity-based knowledge conflicts in question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP’21). 7052–7063.*

[5]*Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT’21). 4812–4829*

[6]*Esin Durmus, He He, and Mona Diab. 2020. FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization. In Proceedings of the 58th Annual Meeting of the ACL. 5055–5070.*

[7]*Nouha Dziri, Hannah Rashkin, Tal Linzen, and David Reitter. 2021. Evaluating groundedness in dialogue systems: The BEGIN benchmark. In Findings of the Association for Computational Linguistics. Association for Computational Linguistics, 1–12.*

[8]*Tanya Goyal and Greg Durrett. 2020. Evaluating factuality in generation with dependency-level entailment. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. 3592–3603.*

[9]*Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. 2018. Wizard of Wikipedia: Knowledge-powered conversational agents. In Proceedings of the International Conference on Learning Representations.*

[10]*Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. 2022. Factuality enhanced language models for open-ended text generation. arXiv preprint arXiv:2206.04624 (2022).*

[11]*Saadia Gabriel, Asli Celikyilmaz, Rahul Jha, Yejin Choi, and Jianfeng Gao. 2021. GO FIGURE: A meta evaluation of factuality in summarization. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Association for Computational Linguistics, 478–487.*
*Or Honovich, Leshem Choshen, Roee Aharoni, Ella Neeman, Idan Szpektor, and Omri Abend. 2021. Q2: Evaluating factual consistency in knowledge-grounded dialogues via question generation and question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 7856–7870*

[12]*Peng, Baolin, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang et al. "Check your facts and try again: Improving large language models with external knowledge and automated feedback."arXiv preprint arXiv:2302.12813(2023).*

[13]*Hannah Rashkin, David Reitter, Gaurav Singh Tomar, and Dipanjan Das. 2021. Increasing faithfulness in knowledgegrounded dialogue with controllable features. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. 704–718.*

[14]Zeqiu Wu, Michel Galley, Chris Brockett, Yizhe Zhang, Xiang Gao, Chris Quirk, Rik Koncel-Kedziorski, et al. 2021. A controllable model of grounded response generation. In Proceedings of the AAAI Conference on Artificial Intelligence. 14085–14093.

[15]*Ratish Puduppully, Li Dong, and Mirella Lapata. 2019. Data-to-text generation with content selection and planning. In Proceedings of the AAAI Conference on Artificial Intelligence*

[16]*Yangming Li, Kaisheng Yao, Libo Qin, Wanxiang Che, Xiaolong Li, and Ting Liu. 2020. Slot-consistent NLG for task-oriented dialogue systems with iterative rectification network. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 97–106.*
*Mohsen Mesgar, Edwin Simpson, and Iryna Gurevych. 2021. Improving factual consistency between a response and persona facts. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics. 549–562.*

[17]*Sihao Chen, Fan Zhang, Kazoo Sone, and Dan Roth. 2021. Improving faithfulness in abstractive summarization with contrast candidate generation and selection. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT’21). 5935–5941.*

[18]《Full Parameter Fine-tuning for Large Language Models with Limited Resources》，2023

[19]《Parameter-Efficient Transfer Learning for NLP》,  2019

[20]《GPT Understands, Too》,  2021

[21]《P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks》,  2022

[22]《LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS》,  2021

[23]《QLORA: Efficient Finetuning of Quantized LLMs》,  2023

[24]《A Survey of Large Language Models》，2023

[25] 《GPT-4 Technical Report》， 2023

[26]《Unifying Large Language Models and Knowledge Graphs: A Roadmap》,  2023

[27] 如何用大语言模型构建一个知识问答系统-腾讯云开发者社区-腾讯云 (tencent.com)

[28]《Search: Query Matching via Lexical, Graph, and Embedding Methods》,  2023
